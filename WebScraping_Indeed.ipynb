{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Search criteria is \n",
      "{'q': 'Data Analyst', 'sort': 'date', 'l': 'NewJersey', 'start': 0, 'explvl': 'entry_level'}\n",
      "The user requested for 3 pages \n",
      "Success in getting main page... https://www.indeed.com/jobs?q=Data+Analyst&sort=date&l=NewJersey&start=0&explvl=entry_level\n",
      "Number of jobs parsed:  13\n",
      "Success in getting page 1...https://www.indeed.com/jobs?q=Data+Analyst&sort=date&l=NewJersey&start=10&explvl=entry_level\n",
      "Number of jobs parsed:  15\n",
      "Success in getting page 2...https://www.indeed.com/jobs?q=Data+Analyst&sort=date&l=NewJersey&start=20&explvl=entry_level\n",
      "Number of jobs parsed:  15\n",
      "(43, 9)\n",
      "File already exists..Do you want to replace[y/n]y\n",
      "Output file written to  C:\\Users\\AnalyticsGirl\\April12NewJersey.csv\n"
     ]
    }
   ],
   "source": [
    "class JobSearch:\n",
    "    global PAGE, HEADERS, NEXT_PAGE_VAL, stopLex\n",
    "    PAGE = 'https://www.indeed.com/jobs?'\n",
    "    HEADERS ={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36' }\n",
    "    stopLex = set(stopwords.words('english'))\n",
    "    NEXT_PAGE_VAL = 10\n",
    "    searchCriteria = {}\n",
    "    jobData = []\n",
    "     \n",
    "    \n",
    "    def __init__(self, q, l, explvl, sort, numPages = 1):\n",
    "        self.searchCriteria['q'] = q\n",
    "        self.searchCriteria['l'] = l\n",
    "        self.searchCriteria['explvl'] = explvl\n",
    "        self.searchCriteria['sort'] = sort\n",
    "        self.searchCriteria['start'] = 0\n",
    "        self.numPages = numPages \n",
    "        \n",
    "    \n",
    "    def getJobDetails(self,link):\n",
    "        html = None\n",
    "        skillDict = {'python':0, 'sql':0, 'masters':0}\n",
    "        keyWords = {}\n",
    "        exp = ['year','years']\n",
    "        nos = ['1','2','3','4','5','6','7','8','9','10','one','two','three','four','five','six','seven','eight','nine','ten']\n",
    "        results = []\n",
    "        title, loc, comp = 'NA', 'NA', 'NA'\n",
    "    \n",
    "        response = requests.get(link,headers = HEADERS)\n",
    "        html = response.content # get the html\n",
    "        \n",
    "        if html:soup = BeautifulSoup(html, 'html.parser')\n",
    "        else:return\n",
    "    \n",
    "        titleChunk = soup.find('b',{'class':'jobtitle'})\n",
    "        if titleChunk: \n",
    "            title = titleChunk.text.encode('ascii','ignore')\n",
    "                \n",
    "        compChunk = soup.find('span',{'class':'company'})\n",
    "        if compChunk: comp = compChunk.text.encode('ascii','ignore')\n",
    "        \n",
    "        locChunk = soup.find('span',{'class':'location'})\n",
    "        if locChunk: loc = locChunk.text.encode('ascii','ignore')\n",
    "        \n",
    "        summary = soup.find('span', {'class':'summary'})\n",
    "        text = summary.text.encode('ascii','ignore')\n",
    "    \n",
    "        text = text.lower().strip()\n",
    "        text = re.sub('[^a-z0-9]',' ',text) # clean\n",
    "    \n",
    "        unfiltered_tokens = word_tokenize(text)\n",
    "        tokens = [token for token in unfiltered_tokens if token not in stopLex and token != ' ']\n",
    "    \n",
    "        fourgrams = ngrams(tokens,4)\n",
    "        experience = [gram for gram in fourgrams if gram[3] in exp and (gram[0] in nos or gram[1] in nos or gram[2] in nos)]\n",
    "    \n",
    "        for word in tokens:\n",
    "                if word in skillDict:skillDict[word]+=1\n",
    "                elif word in keyWords: keyWords[word]+=1\n",
    "                else: keyWords[word]=1\n",
    "    \n",
    "        top3keys = [ i[0] for i in sorted(keyWords.items(), key = lambda x: x[1], reverse = True)[0:3]]\n",
    "        results = [title, comp, loc, skillDict['python'], skillDict['sql'], skillDict['masters'], experience, top3keys, link ]\n",
    "        \n",
    "        return results\n",
    "   \n",
    "    def getJobData(self):\n",
    "        for i in range( self.numPages ):\n",
    "            html = None\n",
    "            response = requests.get(PAGE, headers = HEADERS, params = self.searchCriteria)\n",
    "            html = response.content # get the html\n",
    "\n",
    "            if html: \n",
    "                if i == 0:print 'Success in getting main page...',response.url       \n",
    "                else: print 'Success in getting page %d...%s' %(i,response.url)\n",
    "                self.searchCriteria['start']+=NEXT_PAGE_VAL\n",
    "                \n",
    "            else:\n",
    "                if i == 0:print 'Failure in getting main page...',response.url\n",
    "                else:print 'Failure in getting page %d...%s' %(i,response.url)   \n",
    "                continue\n",
    "        \n",
    "            soup = BeautifulSoup(html, 'html.parser') \n",
    "            jobs = soup.findAll('a',{'data-tn-element':'jobTitle'})\n",
    "            print \"Number of jobs parsed: \",len(jobs)\n",
    "    \n",
    "            for job in jobs:\n",
    "                link = \"http://www.indeed.com\" + job.get('href')\n",
    "                if link:\n",
    "                    self.jobData.append(self.getJobDetails(link))\n",
    "     \n",
    "    def makeJobFile(self,outputLoc):\n",
    "        df = pd.DataFrame(self.jobData)\n",
    "        print df.shape\n",
    "        df.columns = ['JobTitle', 'Company', 'Location', 'Python', 'SQL', 'Masters', 'Experience', 'Keywords', 'JobLink']\n",
    "        now = datetime.datetime.now()\n",
    "        filename = now.strftime(\"%B\")+ str(now.day) + self.searchCriteria['l']\n",
    "        output = outputLoc + filename +'.csv'\n",
    "\n",
    "        if os.path.exists(output):\n",
    "            choice = raw_input('File already exists..Do you want to replace[y/n]')\n",
    "            if choice == 'y':\n",
    "                df.to_csv(output, index = 'False')\n",
    "                print 'Output file written to ',output\n",
    "            else:\n",
    "                print 'File not overwritten'\n",
    "            \n",
    "        else:\n",
    "            df.to_csv(output, index = 'False') \n",
    "            print 'Output file written to ',output\n",
    "    \n",
    "    def printData(self):\n",
    "        print 'The Search criteria is \\n',self.searchCriteria\n",
    "        print 'The user requested for {s} page{plural_s} '.format(s=self.numPages,plural_s=('' if self.numPages == 1 else 's'))\n",
    "                      \n",
    "j1 = JobSearch('Data Analyst','NewJersey','entry_level','date',3) \n",
    "j1.printData()\n",
    "j1.getJobData()\n",
    "output_loc = 'C:\\\\Users\\\\AnalyticsGirl\\\\'\n",
    "j1.makeJobFile(output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
